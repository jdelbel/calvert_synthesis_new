---
title: CTD Fluorometer Correction Workbook
output: html_notebook
---

This worksheet works on created a corrected chlorophyll fluorometer time-series through comparison with discrete chlorophyll samples.

```{r}
#Upload packages
library(tidyverse) #wrangling
library(here) #file management
library(readxl) #reading excel files
library(lubridate) #working with dates and time

#Plotting
library(ggsci) #Nice color schemes
library(patchwork) #plotting panels
# library(egg) #I think this is for panel plotting as well. Still used?

#Interpolation
library(rioja) # add fits to discrete data
```

```{r}
#Downloading baseline corrected chlorophyll fluorescence profiles
f <- read_csv(here("files", "8_binAvg-1762199720021.csv"))

#Flagging file
flags <- read_csv(here("outputs","fluorescence_QC_flags_20251103.csv"))

#Downloading chlorophyll data for joining
c <- read_csv(here("files", "2025-11-03_HakaiData_chlorophyll.csv"))

#Downloading HPLC data for comparison with chlorophyll.
hplc <- read.csv(here("files", "2025-11-18_HakaiData_hplc.csv"))

# Downloading my fit flagging sheet
fit_flag <- read_csv(here("outputs", "ctd_discrete_flags2.csv"))

buoy <- read.csv(here("outputs", "qc_buoy_2025-09-12.csv"))


```

```{r}
#Wrangling CTD profiles, setting date column and renaming columns 
f <- f %>%
  filter(`Cast Direction Flag` == "d") %>%  #downcast data
  mutate(date = lubridate::date(`Measurement time`)) %>%
  mutate(year = lubridate::year(`Measurement time`)) %>%
  select(castpk = `Cast PK`,
         hakai_id = `Hakai ID`,
         Cruise,
         ctdNum = `CTD serial number`,
         station = Station,
         lat = Latitude...11,
         long = Longitude...12,
         time = `Measurement time`,
         date,
         year,
         pres = `Pressure (dbar)`,
         flu = `Fluorometry Chlorophyll (ug/L)`,
         flu_flag = `Fluorometry Chlorophyll flag`,
         par = `PAR (umol m-2 s-1)`,
         turb = `Turbidity (FTU)`)
```

```{r}
#Removing bad or suspect profiles, but leaving shallow casts.
f <- f %>% 
  left_join(flags) %>% 
  filter(flag == "AV") %>% 
  filter(station %in% c("KC10", "FZH01"))
```

```{r}
#Looking at days with replicate casts and looking at the time difference between them
num_dup <- f %>% 
  filter(pres == 5) %>%
  group_by(date, station) %>% 
  summarise(n_prof = n(),
            min_time = min(time),
            max_time = max(time)) %>% 
  ungroup() %>% 
  filter(n_prof > 1) %>% 
  mutate(diff_time = difftime(max_time, min_time, units = "hours"))

#Only a couple with 3+ hours. 

#2018-07-15-KC10 - Pretty comparable despite time difference.
#SVD because of mismatches - I wonder if picking closest would make better?

#2017-08-31-FZH01 - Some differences here for-sure.
#variability is largely in the surface waters and I did have to cut these out.

#Definitely needs to be some consideration about these joins.
```
```{r}
# f %>% 
#   filter(date == "2017-08-31" & station == "FZH01") %>% 
#   ggplot(aes(x = flu, y = pres, color = as.factor(castpk))) +
#   geom_line(orientation = "y") +
#   scale_y_reverse(lim = c(50, 0))
```

```{r}
# Calculate dark offsets from deep water (>100m)
dark_offsets <- f %>%
  filter(pres > 100) %>%
  group_by(date, station, ctdNum) %>%
  mutate(n = n()) %>% 
  ungroup() %>% 
  filter(n >= 10) %>% 
  group_by(date, station, ctdNum) %>%
  slice_min(flu, n = 10) %>% 
  summarise(
    dark_offset = median(flu, na.rm = TRUE),
    dark_offset_sd = sd(flu, na.rm = TRUE),
    max_depth = max(pres)
  ) %>%
  ungroup()

#Joining dark offsets into the fluorescence profiles
f <- f %>% 
  left_join(dark_offsets)

# Looking at early time-series casts that did not go deep enough to derive a deep dark offset - can shallower data be used?
shallow_dark <- f %>% 
  filter(is.na(dark_offset)) %>%
  filter(pres > 10) %>% 
  group_by(castpk, date, station, ctdNum) %>% 
  slice_min(flu, n = 10) %>% 
  summarise(max_depth = max(pres),
            shallow_offset = median(flu),
            shallow_offset_sd = sd(flu),
            .groups = "drop")
```

```{r}
#Plotting only the deep (>100m) dark offsets.
dark_offsets %>% 
  ggplot(aes(x = date, y = dark_offset, color = as.factor(ctdNum))) +
  geom_errorbar(aes(ymin = dark_offset - dark_offset_sd, 
                    ymax = dark_offset + dark_offset_sd),
                width = 0, alpha = 0.4) +
  geom_point(size = 2) +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.01)) +
  scale_color_brewer(palette = "Dark2", name = "CTD Sensor") +
  labs(
    x = "Date",
    y = expression(paste("Dark offset (mg ", m^{-3}, ")")),
    title = "Fluorometer dark offsets from deep water (>100 m)"
  ) +
  facet_grid(ctdNum ~ ., scales = "free_y") +
  theme_bw() +
  theme(
    legend.position = "none",
    strip.background = element_rect(fill = "grey90"),
    panel.grid.minor = element_blank(),
    text = element_text(size = 28, color = "black"),
    axis.title.x = element_blank()
  )

ggsave(here("figures", "dark_offsets.png"), 
       width = 14,
       height = 8,
       dpi = 300)
```

```{r}
#Plotting the dark offsets with the shallow casts
dark_offsets %>% 
  ggplot(aes(x = date, y = dark_offset, color = as.factor(ctdNum))) +
  # Deep offsets
  geom_errorbar(aes(ymin = dark_offset - dark_offset_sd, 
                    ymax = dark_offset + dark_offset_sd),
                width = 0, alpha = 0.4) +
  geom_point(size = 3) +
  # Shallow offsets
  geom_errorbar(data = shallow_dark,
                aes(y = shallow_offset,
                    ymin = shallow_offset - shallow_offset_sd, 
                    ymax = shallow_offset + shallow_offset_sd),
                width = 0, alpha = 0.4) +
  geom_point(data = shallow_dark,
             aes(y = shallow_offset),
             size = 3, shape = 21, fill = "white", stroke = 1.2) +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.01)) +
  scale_color_brewer(palette = "Dark2", name = "CTD Sensor") +
  labs(
    x = "Date",
    y = expression(paste("Dark offset (mg ", m^{-3}, ")")),
    title = "Fluorometer dark offsets\nDeep (>100 m, filled) vs shallow (>10 m, open)"
  ) +
  facet_grid(ctdNum ~ ., scales = "free_y") +
  theme_bw() +
  theme(
    legend.position = "none",
    strip.background = element_rect(fill = "grey90"),
    panel.grid.minor = element_blank(),
    text = element_text(size = 28, color = "black"),
    axis.title.x = element_blank()
  )

ggsave(here("figures", "dark_offsets_shallow.png"), 
       width = 14,
       height = 8,
       dpi = 300)
```

```{r}
#Investigate the few profiles where there are high offsets
#80217 > 1

# Identify high offset profiles for this sensor
check_dark_80217 <- dark_offsets %>% 
  filter(ctdNum == "80217", dark_offset > 1)

# Filter data for context profiles
context_profiles <- f %>% 
  filter(date <= "2014-10-16", ctdNum == 80217)

depth_limit <- max(context_profiles$pres, na.rm = TRUE)

# Main plot: all profiles for context
p1 <- context_profiles %>% 
  ggplot(aes(x = flu, y = pres, color = as.factor(date))) +
  geom_line(orientation = "y", linewidth = 0.8) +
  scale_y_reverse(limits = c(depth_limit, 0)) +
  scale_color_viridis_d(option = "turbo", name = "Date") +
  labs(
    x = expression(paste("Fluorescence (mg ", m^{-3}, ")")),
    y = "Pressure (dbar)",
    title = "CTD 80217 profiles"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    text = element_text(size = 28, color = "black"),
  ) +
  guides(color = guide_legend(nrow = 2))

# Highlight plot: the high offset profile
p2 <- context_profiles %>% 
  filter(date == "2014-10-16", station == "FZH01") %>% 
  ggplot(aes(x = flu, y = pres)) +
  geom_line(orientation = "y", linewidth = 1, color = "#D55E00") +
  scale_y_reverse(limits = c(depth_limit, 0)) +
  labs(
    x = expression(paste("Fluorescence (mg ", m^{-3}, ")")),
    y = NULL,
    title = "2014-10-16 FZH01"
  ) +
  theme_bw() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    text = element_text(size = 28, color = "black"),
  )

# Combine plots
library(patchwork)
p1 + p2 + plot_layout(widths = c(1, 1))

ggsave(here("figures", "dark_offset_check_80217.png"),
       width = 12,
       height = 10,
       dpi = 300)
```

```{r}
# investigating high dark offset profiles for 18066
check_dark_18066 <- dark_offsets %>% 
  filter(ctdNum == "18066", 
         date >= "2014-03-26",
         date <= "2014-04-21")

# Get dates with high offsets
high_offset_dates <- check_dark_18066 %>% 
  filter(dark_offset > 0.08) %>% 
  pull(date)

# Filter data for context profiles
context_profiles <- f %>% 
  filter(ctdNum == 18066,
         date >= "2014-03-26",
         date <= "2014-04-21")

# Get shared depth limit
depth_limit <- max(context_profiles$pres, na.rm = TRUE)

# Main plot: all profiles for context
p1 <- context_profiles %>% 
  ggplot(aes(x = flu, y = pres, color = as.factor(castpk))) +
  geom_line(orientation = "y", linewidth = 0.8) +
  scale_y_reverse(limits = c(depth_limit, 0)) +
  scale_color_viridis_d(option = "turbo", name = "Date") +
  labs(
    x = expression(paste("Fluorescence (mg ", m^{-3}, ")")),
    y = "Pressure (dbar)",
    title = "CTD 18066 profiles"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 8)
  ) +
  guides(color = guide_legend(nrow = 2))

# Highlight plot: high offset profiles
p2 <- context_profiles %>% 
  filter(date %in% high_offset_dates) %>% 
  ggplot(aes(x = flu, y = pres, color = as.factor(castpk))) +
  geom_line(orientation = "y", linewidth = 1) +
  scale_y_reverse(limits = c(depth_limit, 0)) +
  scale_color_brewer(palette = "Dark2", name = "Date") +
  labs(
    x = expression(paste("Fluorescence (mg ", m^{-3}, ")")),
    y = NULL,
    title = "Dark offset > 0.08"
  ) +
  theme_bw() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    legend.position = "bottom"
  )

# Combine plots
library(patchwork)
p1 + p2 + plot_layout(widths = c(1, 1))

ggsave(here("figures", "dark_offset_check_18066.png"),
       width = 12, height = 10, dpi = 300)
```

```{r}
#Fill early time-series shallow casts with closes deep dark offset - should be ok as sensors this is being done to are relatively stable and show low variability. Could introduce error.
f <- f %>% 
  mutate(dark_offset_filled = is.na(dark_offset)) %>% 
  arrange(ctdNum, date) %>% 
  group_by(ctdNum) %>% 
  fill(dark_offset, dark_offset_sd, .direction = "downup") %>% 
  ungroup()

f_offset <- f %>% 
  mutate(f_offset = pmax(flu - dark_offset, 0))
```

```{r}
#For now, I think best to just do a daily mean on fluorescence profiles, but this needs evaluation
f_dm <- f_offset %>% 
  group_by(date, station, pres, ctdNum) %>% 
  summarise(f_dm = mean(f_offset),
            f_sd = sd(f_offset),
            n_prof = n()) %>% 
  ungroup() %>% 
  mutate(f_dm = round(f_dm, 2),
         f_lower = f_dm - f_sd, 
         f_upper = f_dm + f_sd) %>% 
  unite(id, c(date, station, ctdNum), sep = "-", remove = F)
```
```{r}
# Calculate dark offsets from deep water (>100m)
dark_offsets <- f_dm %>%
  filter(pres > 100) %>%
  group_by(date, station, ctdNum, id) %>%
  mutate(n = n()) %>% 
  ungroup() %>% 
  filter(n >= 10) %>% 
  group_by(date, station, ctdNum, id) %>%
  slice_min(n = 10)
  
  
  arrange(f_dm) %>%
  slice_head(n = 10) %>%  # Take 10 minimum values
  summarise(
    dark_offset = mean(f_dm, na.rm = TRUE),
    dark_offset_sd = sd(f_dm, na.rm = TRUE),
    n_deep = n(),
    max_depth = max(pres)
  ) %>%
  ungroup()

# Define temporal periods for sensors with clear shifts
offset_periods <- tribble(
  ~ctdNum, ~period_start, ~period_end, ~period_name,
  18066, as.Date("2012-01-01"), as.Date("2014-08-01"), "early",
  18066, as.Date("2016-01-16"), as.Date("2017-01-20"), "middle",
  18066, as.Date("2017-08-15"), as.Date("2019-10-04"), "late",
  80217, as.Date("2012-01-01"), as.Date("2014-10-16"), "early",
  80217, as.Date("2019-11-25"), as.Date("2022-02-06"), "late",
  18032, as.Date("2012-01-01"), as.Date("2030-01-01"), "all",
  211567, as.Date("2012-01-01"), as.Date("2030-01-01"), "all",
  1907467, as.Date("2012-01-01"), as.Date("2030-01-01"), "all"
)

# Calculate single offset value for each sensor/period (median of deep measurements)
offset_lookup <- dark_offsets %>%
  left_join(offset_periods, by = "ctdNum") %>%
  filter(date >= period_start & date <= period_end) %>%
  group_by(ctdNum, period_name, period_start, period_end) %>%
  summarise(
    offset = median(dark_offset, na.rm = TRUE),
    offset_sd = sd(dark_offset, na.rm = TRUE),
    n_profiles = n(),
    .groups = "drop"
  )

# View the lookup table
offset_lookup %>%
  arrange(ctdNum, period_start)

# Save the offset lookup table
# write_csv(offset_lookup, here("output", "dark_offset_lookup.csv"))


# Apply offsets to f_dm data
f_dm <- f_dm %>%
  left_join(offset_periods, by = "ctdNum", relationship = "many-to-many") %>%
  filter(date >= period_start & date <= period_end) %>%
  left_join(offset_lookup %>% select(ctdNum, period_name, offset), 
            by = c("ctdNum", "period_name")) %>%
  # Keep only one row per original f_dm row (remove duplicates from many-to-many join)
  distinct(id, pres, .keep_all = TRUE) %>%
  mutate(f_corrected = f_dm - offset) %>%
  select(date, station, pres, ctdNum, id, f_dm, offset, f_corrected, everything(), 
         -period_start, -period_end, -period_name)

# Simple visualization
p_offsets <- offset_lookup %>%
  ggplot(aes(x = period_name, y = offset, fill = period_name)) +
  geom_col() +
  geom_errorbar(aes(ymin = offset - offset_sd, ymax = offset + offset_sd),
                width = 0.2) +
  geom_text(aes(label = sprintf("%.3f\n(n=%d)", offset, n_profiles)),
            vjust = -0.5, size = 3) +
  facet_wrap(~ctdNum, scales = "free",
             labeller = labeller(ctdNum = ~paste("Sensor", .))) +
  labs(
    x = "Period",
    y = "Dark Offset (fluorescence units)",
    title = "Dark Offset Values by Sensor and Period",
    subtitle = "Median of deep water (>100m) measurements; error bars show ±1 SD"
  ) +
  theme_bw(base_size = 14) +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = "gray90"),
    strip.text = element_text(size = 12, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

p_offsets

ggsave(here("figures", "dark_offset_lookup.png"),
       plot = p_offsets,
       width = 10, height = 6, dpi = 300, bg = "white")
```




```{r}
#Working with the discrete chlorophyll dataset

#Pulling out bulk data with appropriate flags and running a daily mean in case there are duplicates
#Allowing SVC and ADL as generally OK.
c_qc <- c %>% 
  select(date, station = site_id, line_out_depth, filter_type, chla, chla_flag) %>% 
  filter(filter_type == "Bulk GF/F") %>% 
  filter(chla_flag %in% c("AV", "SVC", "ADL") | is.na(chla_flag)) 

#Calculating a daily mean value in case of duplicates and setting the 0m sampling depth to 1m to match with the closest CTD fluorometer record
c_dm <- c_qc %>% 
  group_by(date, station, line_out_depth) %>% 
  summarise(chl_dm = mean(chla)) %>% 
  ungroup() %>% 
  mutate(pres = round(line_out_depth)) %>%
  drop_na() %>% 
  group_by(date, station) %>% 
  mutate(n_dep = n()) %>% 
  ungroup() %>% 
  # filter(n_dep >= 3) %>% #This could be altered to > 3x points maybe?
  mutate(year = year(date)) %>% 
  mutate(pres = case_when(pres == 0 ~ 1,
                          TRUE ~ as.numeric(pres)))

#Pulling out size-fractionated data
c_sf <- c %>% 
  select(date, station = site_id, line_out_depth, filter_type, chla, chla_flag) %>% 
  filter(!filter_type == "Bulk GF/F") %>% 
  filter(chla_flag == "AV" | chla_flag == "SVC" | chla_flag == "ADL" | is.na(chla_flag))

#Calculating a daily mean value where there are three filters available to complete the set.
c_sf_dm <- c_sf %>% 
  filter(!is.na(chla)) %>%
  filter(chla > 0) %>% 
  group_by(date, station, line_out_depth, filter_type) %>% 
  summarise(avg_chla = mean(chla)) %>%
  ungroup() %>% 
  group_by(date, station, line_out_depth) %>% 
  mutate(n_filt = n()) %>% 
  ungroup() %>% 
  group_by(date, station, line_out_depth, filter_type) %>% 
  mutate(n_type = n()) %>% 
  ungroup() %>% 
  filter(n_filt == 3 & n_type == 1) %>% 
  group_by(date, station, line_out_depth) %>% 
  mutate(sum = sum(avg_chla)) %>% 
  ungroup() %>% 
  mutate(perc = avg_chla/sum) %>% 
  select(date, station, pres = line_out_depth, filter_type, avg_chla, sum, perc) %>% 
  mutate(filter_type2 = case_when(filter_type == "2um" ~ "3um",
                                  TRUE ~ as.character(filter_type)))

#Setting the 0m sampling depth to 1m
c_sum <- c_sf_dm %>% 
  distinct(sum, .keep_all = T) %>% 
  select(date, station, pres, sum) %>% 
  mutate(pres = case_when(pres == 0 ~ 1,
                          TRUE ~ as.numeric(pres)))
```

```{r}
#Joining the bulk and size-fractionated sum and then creating combined column where the size-fractionated value is used where there are NA's for the bulk.
c_join <- c_dm %>%
  full_join(c_sum, by = c("date", "station", "pres")) %>%
  mutate(chl_comb = case_when(
    is.na(chl_dm) ~ sum,
    !is.na(chl_dm) ~ chl_dm
  )) %>%
  # Recalculate n_dep based on combined data
  group_by(date, station) %>%
  mutate(n_dep = n()) %>%
  ungroup() %>%
  select(date, station, pres, n_dep, sum, chl_dm, chl_comb) %>%
  mutate(diff = round(chl_dm - sum, 2))

# c_join <- c_dm %>% 
#   full_join(c_sum, by = c("date", "station", "pres")) %>%
#   mutate(chl_comb = case_when(
#     date == as.Date("2021-09-05") & station == "KC10" ~ sum,  # Special case
#     is.na(chl_dm) ~ sum,
#     !is.na(chl_dm) ~ chl_dm
#   )) %>%
#   # Recalculate n_dep based on combined data
#   group_by(date, station) %>%
#   mutate(n_dep = n()) %>%
#   ungroup() %>%
#   select(date, station, pres, n_dep, sum, chl_dm, chl_comb) %>% 
#   mutate(diff = round(chl_dm - sum, 2))
# %>% 
#   unite(id, c(date, station), sep = "-", remove = FALSE)
```

```{r}
# Remove NAs and classify outliers based on difference
c_join_comp <- c_join %>%
  filter(!is.na(chl_dm) & !is.na(sum)) %>%
  mutate(outlier = abs(chl_dm - sum) >= 5)

# Calculate regression statistics on non-outliers only
c_join_clean <- c_join_comp %>% filter(!outlier)
model_chl_comp <- lm(sum ~ chl_dm, data = c_join_clean)
r_squared <- summary(model_chl_comp)$r.squared
slope <- coef(model_chl_comp)[2]
intercept <- coef(model_chl_comp)[1]
n_clean <- nrow(c_join_clean)
n_total <- nrow(c_join)
n_outliers <- sum(c_join$outlier)

# Determine axis limits
max_val <- max(c(c_join$chl_dm, c_join$sum), na.rm = TRUE)
min_val <- min(c(c_join$chl_dm, c_join$sum), na.rm = TRUE)

# Create plot
c_join_comp %>% 
  ggplot(aes(x = chl_dm, y = sum)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", 
              color = "gray50", linewidth = 0.5) +
  geom_smooth(data = c_join_clean, method = "lm", se = TRUE, 
              color = "#2166ac", fill = "#2166ac", alpha = 0.2) +
  geom_point(aes(color = outlier), size = 3, alpha = 0.6) +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "#d73027"),
                     labels = c("FALSE" = "Included", "TRUE" = "Outlier"),
                     name = NULL) +
  annotate("text", x = min_val + 0.1 * (max_val - min_val), 
           y = max_val - 0.1 * (max_val - min_val),
           label = sprintf("y = %.2fx + %.2f\nR² = %.3f\nn = %d (%d outliers)", 
                          slope, intercept, r_squared, n_clean, n_outliers),
           hjust = 0, size = 3.5) +
  coord_fixed(xlim = c(min_val, max_val), ylim = c(min_val, max_val)) +
  labs(x = expression("Chl"[BULK]~"(mg m"^-3*")"),
       y = expression("Chl"[SF-SUM]~"(mg m"^-3*")")) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        legend.position = c(0.85, 0.15))

ggsave(here("figures", "chl_comparison.png"),
       width = 6, height = 6, units = "in",
       dpi = 300, bg = "white")
```
```{r}
#large mismatches - High SF
#2022-03-09-KC10 sum much higher than bulk (37 vs 11 @ 1m) - flu saturated.
#2023-05-14-KC10 17vs7 @20m - SVD 2023 issue profile where 20m SF likely more rep.
#2023-06-15-KC10 14vs4 @5m - SVD due to narrow SCM - SF might be more rep, but HPLC 8
#2023-06-15-KC10 11vs4 @1m
#For the above can try switching with SF sum rather than bulk to see if improves.
#2022-07-28-KC10 25vs20 @10m  SVD, but because of 5m not 10m - doesn't help.

#High Bulk.
#2018-07-15-FZH01 1vs24@10m - removed from profile fit as clear outlier - sf not better
#2018-07-15-KC10  7vs29@10m - 10m depth mismatch, slope SVD, sf not better.
#2016-05-26-FZH01 16vs25@5m -  narrow scm, bulk more rep.
#2021-09-05-KC10  4vs12@1m - I think SF sum more rep - try - currently blame NPQ
#2021-09-05-KC10  8vs14@5m - I think SF sum more rep - try - currently blame NPQ
#2022-07-28-KC10  28vs33@5m - helps a bit, but not really. HPLC 13
#2022-06-03-KC10 17vs23@10m - SVD slope very steep due to 10m, sf diff not enough.
#2017-06-29-FZH01 11vs16@10m - Can't find a cast
#2022-07-03-KC10  15vs20@10m -sf would be better, 20m still outlier but could remove.  

# filter(date == "2022-03-09" & station == "KC10" & pres == 1)
# filter(date == "2023-05-14" & station == "KC10" & pres == 20)
# filter(date == "2023-06-15" & station == "KC10" & pres == 5)
# filter(date == "2023-06-15" & station == "KC10" & pres == 1)
# filter(date == "2022-07-28" & station == "KC10" & pres == 10)
# filter(date == "2018-07-15" & station == "FZH01" & pres == 10)
# filter(date == "2018-07-15" & station == "KC10" & pres == 10)
# filter(date == "2016-05-26" & station == "FZH01" & pres == 5)
# filter(date == "2021-09-05" & station == "KC10" & pres == 1)
# filter(date == "2021-09-05" & station == "KC10" & pres == 5)
# filter(date == "2022-07-28" & station == "KC10" & pres == 5)
# filter(date == "2022-06-03" & station == "KC10" & pres == 10)
# filter(date == "2017-06-29" & station == "FZH01" & pres == 10)
# filter(date == "2022-07-03" & station == "KC10" & pres == 10)


# test <- c_join %>% 
#   filter(date == "2022-03-09")
```

```{r}
hplc <- hplc %>%  
  mutate(date = ymd(date)) %>% 
  group_by(date, site_id, line_out_depth) %>% 
  summarise(tchl_dm = mean(all_chl_a)) %>% 
  ungroup() %>% 
  mutate(pres = round(line_out_depth)) %>%
  drop_na() %>% 
  group_by(date, site_id) %>% 
  mutate(n_dep = n()) %>% 
  ungroup() %>% 
  # filter(n_dep >= 3) %>% #This could be altered to > 3x points maybe?
  mutate(year = year(date)) %>% 
  mutate(pres = case_when(pres == 0 ~ 1,
                          TRUE ~ as.numeric(pres))) %>% 
  rename(station = site_id)
```
```{r}
# Join HPLC data to c_join, keeping only non-outliers from previous step
c_join_hplc <- c_join_comp %>%
  filter(!outlier) %>%  # Remove the outliers identified in chl_dm-sum comparison
  left_join(hplc, by = c("date", "station", "pres")) %>%
  filter(!is.na(tchl_dm) & !is.na(chl_comb))

# Calculate regression statistics
model_hplc <- lm(chl_comb ~ tchl_dm, data = c_join_hplc)
r_squared_hplc <- summary(model_hplc)$r.squared
slope_hplc <- coef(model_hplc)[2]
intercept_hplc <- coef(model_hplc)[1]
n_hplc <- nrow(c_join_hplc)

# Determine axis limits
max_val_hplc <- max(c(c_join_hplc$tchl_dm, c_join_hplc$chl_comb), na.rm = TRUE)
min_val_hplc <- min(c(c_join_hplc$tchl_dm, c_join_hplc$chl_comb), na.rm = TRUE)

# Create plot
p_hplc <- c_join_hplc %>% 
  ggplot(aes(x = tchl_dm, y = chl_comb)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", 
              color = "gray50", linewidth = 0.5) +
  geom_smooth(method = "lm", se = TRUE, 
              color = "#2166ac", fill = "#2166ac", alpha = 0.2) +
  geom_point(size = 3, alpha = 0.6) +
  annotate("text", x = min_val_hplc + 0.1 * (max_val_hplc - min_val_hplc), 
           y = max_val_hplc - 0.1 * (max_val_hplc - min_val_hplc),
           label = sprintf("y = %.2fx + %.2f\nR² = %.3f\nn = %d", 
                          slope_hplc, intercept_hplc, r_squared_hplc, n_hplc),
           hjust = 0, size = 3.5) +
  coord_fixed(xlim = c(min_val_hplc, max_val_hplc), 
              ylim = c(min_val_hplc, max_val_hplc)) +
  labs(x = expression("Chl"[HPLC]~"(mg m"^-3*")"),
       y = expression("Chl"[COMB]~"(mg m"^-3*")")) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        axis.title = element_text(size = 11),
        axis.text = element_text(size = 10))

# Display plot
p_hplc

# Save plot
ggsave(here("figures", "chl_hplc_comparison.png"),
       plot = p_hplc,
       width = 6, height = 6, units = "in",
       dpi = 300, bg = "white")
```



```{r}
#I should build an offset to correct for the slight difference between the two measures.

#Joining the full discrete chlorophyll dataset to the daily mean fluorometer dataset.
f_dm <- f_dm %>% 
  left_join(c_join)

#A comparison of the bulk and SF sum should be plotted and offset applied if needed.
```

```{r}
f_dm %>% 
  select(date, station) %>% 
  distinct() %>% 
  ggplot(aes(x = date, y = station, color = station)) +
  geom_point()
```

```{r}
#Creatiing a dataset to apply linear fits between the chlorophyll and fluorometry data to derive a slope correction.

#I am manually removing large outliers determined from visual inspection. They are all typically from the discrete sample missing a narrow layer below.
f_fit <- f_dm %>%
  left_join(c_join) %>% 
  filter(!is.na(chl_comb)) %>% 
  unite(id, c(date, station, ctdNum), sep = "-", remove = F)  

# Then after merging with CTD data to create f_fit:
# Separate into multi-sample (>=3) and few-sample (1-2) casts
#Notes on these outlier removals are in excel sheet
f_fit_multi <- f_fit %>%
  filter(!is.na(chl_comb) & !is.na(f_corrected)) %>%
  group_by(date, station, ctdNum) %>%
  filter(n() >= 3) %>%  # Only keep casts with 3+ samples for cast-specific fits
  ungroup() %>%
  filter(!(id == "2023-03-18-KC10-18032" & pres == 30)) %>% 
  filter(!(id == "2014-06-19-KC10-18066" & pres == 5)) %>% 
  filter(!(id == "2017-08-31-KC10-18066" & pres == 1)) %>% 
  filter(!(id == "2018-07-15-FZH01-18066" & pres == 10)) %>% 
  filter(!(id == "2017-04-28-FZH01-1907467" & pres == 10)) %>% 
  filter(!(id == "2017-07-19-FZH01-1907467" & pres == 20)) %>% 
  filter(!(id == "2017-09-21-FZH01-1907467" & pres == 10))
  
f_fit_few <- f_fit %>%
  filter(!is.na(chl_comb) & !is.na(f_corrected)) %>%
  group_by(date, station, ctdNum) %>%
  filter(n() < 3) %>%   # Casts with 1-2 samples
  ungroup()
```


Need to go through and provide rationale why I'm removing things above.

```{r}
# Fit both models (with and without surface) and select the best slope for a correction based on p-value and r2. When the surface value is not used, it generally means that the fluorometer data are NPQ affected, so a identifier column in put in for this.
#Code to apply fits and extract statistics - used Claude to streamline the method that I derived.
model <- f_fit_multi %>% 
  group_by(date, station, ctdNum) %>%
  summarise({
    # Check data availability
    data_no_surf <- filter(cur_data(), pres != 1)
    data_with_surf <- cur_data()
    
    nobs_no_surf <- nrow(data_no_surf)
    nobs_with_surf <- nrow(data_with_surf)
    
    # Initialize variables
    can_fit_no_surf <- nobs_no_surf >= 3
    can_fit_with_surf <- nobs_with_surf >= 3
    
    # Fit model WITHOUT surface (if possible)
    if (can_fit_no_surf) {
      model_no_surf <- lm(chl_comb ~ f_corrected, data = data_no_surf)
      intercept_no_surf <- round(coef(model_no_surf)[1], 2)
      slope_no_surf <- round(coef(model_no_surf)[2], 2)
      r2_no_surf <- round(summary(model_no_surf)$adj.r.squared, 2)
      p.value_no_surf <- round(summary(model_no_surf)$coefficients[2, 4], 5)
    } else {
      intercept_no_surf <- slope_no_surf <- r2_no_surf <- p.value_no_surf <- NA
    }
    
    # Fit model WITH surface (if possible)
    if (can_fit_with_surf) {
      model_with_surf <- lm(chl_comb ~ f_corrected, data = data_with_surf)
      intercept_with_surf <- round(coef(model_with_surf)[1], 2)
      slope_with_surf <- round(coef(model_with_surf)[2], 2)
      r2_with_surf <- round(summary(model_with_surf)$adj.r.squared, 2)
      p.value_with_surf <- round(summary(model_with_surf)$coefficients[2, 4], 5)
    } else {
      intercept_with_surf <- slope_with_surf <- r2_with_surf <- p.value_with_surf <- NA
    }
    
    # Select best model
    if (!can_fit_no_surf & !can_fit_with_surf) {
      # Neither model can be fit
      use_surface <- NA
      intercept <- slope <- r2 <- p.value <- nobs <- NA
    } else if (!can_fit_no_surf) {
      # Only surface model can be fit
      use_surface <- TRUE
      intercept <- intercept_with_surf
      slope <- slope_with_surf
      r2 <- r2_with_surf
      p.value <- p.value_with_surf
      nobs <- nobs_with_surf
    } else if (!can_fit_with_surf) {
      # Only no-surface model can be fit
      use_surface <- FALSE
      intercept <- intercept_no_surf
      slope <- slope_no_surf
      r2 <- r2_no_surf
      p.value <- p.value_no_surf
      nobs <- nobs_no_surf
    } else {
      # Both models can be fit - choose best based on p-value
      use_surface <- p.value_with_surf < p.value_no_surf
      intercept <- ifelse(use_surface, intercept_with_surf, intercept_no_surf)
      slope <- ifelse(use_surface, slope_with_surf, slope_no_surf)
      r2 <- ifelse(use_surface, r2_with_surf, r2_no_surf)
      p.value <- ifelse(use_surface, p.value_with_surf, p.value_no_surf)
      nobs <- ifelse(use_surface, nobs_with_surf, nobs_no_surf)
    }
    
    # Determine if surface is quenched (available but not used)
    surface_available <- any(data_with_surf$pres == 1)
    quenched <- surface_available && !isTRUE(use_surface)
    
    data.frame(
      intercept = intercept,
      slope = slope,
      r2 = r2,
      p.value = p.value,
      nobs = nobs,
      use_surface = use_surface,
      quenched = quenched
    )
  }, .groups = 'drop') %>% 
  unite(id, c(date, station, ctdNum), sep = "-", remove = F) %>%
  arrange(date, station, ctdNum) %>%
  mutate(method = "cast_specific")
```

```{r}
# First, join the model results back to f_fit to get ctdNum
f_fit_with_model <- f_fit_multi %>%
  left_join(model %>% select(id,
                             intercept,
                             slope,
                             r2,
                             p.value,
                             use_surface,
                             quenched,
                             nobs),
            by = "id") %>%
  filter(!is.na(chl_comb) & !is.na(f_corrected))
```

```{r}

# Get median slope and intercept by sensor
sensor_medians <- f_fit_with_model %>%
  filter(!is.na(slope)) %>%
  group_by(ctdNum) %>%
  summarise(
    median_slope = median(slope, na.rm = TRUE),
    max_slope = max(slope, na.rm = TRUE),
    min_slope = min(slope, na.rm = TRUE),
    median_intercept = median(intercept, na.rm = TRUE),
    max_intercept = max(intercept, na.rm = TRUE),
    min_intercept = min(intercept, na.rm = TRUE),
  )
```

```{r}
f_fit_with_model <- f_fit_with_model %>%
  left_join(sensor_medians, by = "ctdNum")

ggplot(f_fit_with_model, aes(x = f_corrected, y = chl_comb)) +
  geom_abline(slope = 1, intercept = 0, 
              linetype = "dashed", color = "gray40", linewidth = 0.6) +
  geom_abline(aes(slope = median_slope, intercept = median_intercept),
              color = "blue", linewidth = 1) +
  geom_smooth(aes(group = id), method = "lm", se = TRUE, 
              linewidth = 0.4, alpha = 0.3, color = "steelblue") +
  geom_point(size = 2, alpha = 0.6) +
  facet_wrap(~ctdNum, scales = "free",
             labeller = labeller(ctdNum = ~paste("Sensor", .))) +
  labs(
    x = "CTD Fluorescence (f_dm)",
    y = expression(paste("Discrete Chlorophyll (mg m"^-3, ")")),
    title = "CTD Fluorescence Calibrations by Sensor",
    subtitle = "Each line represents one cast's linear regression; dashed line shows 1:1"
  ) +
  theme_bw(base_size = 11) +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = "gray90"),
    aspect.ratio = 1
  )

# Save with better dimensions
ggsave(here("figures", "ctd_fluorescence_calibration_by_sensor_all_dark.png"),
       width = 10,
       height = 6.5,
       dpi = 300,
       bg = "white")
```


```{r}
fit_flag_clean <- fit_flag %>% 
  unite(id, c(id, ctdNum), sep = "-", remove = F) %>% 
  select(id,
         slope_flag = flag)

f_fit_av <- f_fit_with_model %>% 
  left_join(fit_flag_clean) %>% 
  filter(slope_flag == "AV")

# Calculate overall fit statistics for each sensor
fit_stats <- f_fit_av %>% 
  filter(use_surface | pres != 1) %>%
  group_by(ctdNum) %>%
  summarise(
    model = list(lm(chl_comb ~ f_corrected)),
    .groups = 'drop'
  ) %>%
  mutate(
    slope = map_dbl(model, ~coef(.x)[2]),
    intercept = map_dbl(model, ~coef(.x)[1]),
    r2 = map_dbl(model, ~summary(.x)$r.squared),
    adj_r2 = map_dbl(model, ~summary(.x)$adj.r.squared),
    p_value = map_dbl(model, ~summary(.x)$coefficients[2, 4]),
    n = map_int(model, ~length(.x$residuals)),
    label = sprintf("y = %.2fx + %.2f\nR² = %.3f (adj. %.3f)\np < %.4f\nn = %d",
                    slope, intercept, r2, adj_r2, p_value, n)
  )

f_fit_av %>% 
  filter(use_surface | pres != 1) %>%
  ggplot(aes(x = f_corrected, y = chl_comb)) +
  geom_abline(slope = 1, intercept = 0, 
              linetype = "dashed", color = "gray40", linewidth = 0.8) +
  geom_smooth(aes(group = id), method = "lm", se = TRUE, 
              linewidth = 0.5, alpha = 0.3, color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, 
              linewidth = 1.5, alpha = 0.3, color = "black") +
  geom_point(size = 3, alpha = 0.6) +
  geom_text(data = fit_stats,
            aes(x = -Inf, y = Inf, label = label),
            hjust = -0.05, vjust = 1.1,
            size = 4.5, fontface = "italic",
            inherit.aes = FALSE) +
  facet_wrap(~ctdNum, scales = "free",
             labeller = labeller(ctdNum = ~paste("Sensor", .))) +
  labs(
    x = "CTD Fluorescence (f_dm)",
    y = expression(paste("Discrete Chlorophyll (mg m"^-3, ")")),
    title = "CTD Fluorescence Calibrations by Sensor",
    subtitle = "Each blue line represents one cast's linear regression; black line shows overall fit; dashed line shows 1:1"
  ) +
  theme_bw(base_size = 16) +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = "gray90"),
    strip.text = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 18, face = "bold"),
    plot.subtitle = element_text(size = 14),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    aspect.ratio = 1
  )

ggsave(here("figures", "fluorescence_calibration_by_sensor.png"),
       width = 12, height = 10, dpi = 300)
```

```{r}
# Calculate fit statistics with dual regression for sensor 80217
fit_stats <- f_fit_av %>% 
  filter(use_surface | pres != 1) %>%
  group_by(ctdNum) %>%
  summarise(
    # Overall model
    model = list(lm(chl_comb ~ f_corrected)),
    
    # Low range model for sensor 80217 only
    slope_low = if(first(ctdNum) == "80217") {
      coef(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% filter(chl_comb < 1)))[2]
    } else {
      NA_real_
    },
    intercept_low = if(first(ctdNum) == "80217") {
      coef(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% filter(chl_comb < 1)))[1]
    } else {
      NA_real_
    },
    r2_low = if(first(ctdNum) == "80217") {
      summary(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% filter(chl_comb < 1)))$r.squared
    } else {
      NA_real_
    },
    adj_r2_low = if(first(ctdNum) == "80217") {
      summary(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% filter(chl_comb < 1)))$adj.r.squared
    } else {
      NA_real_
    },
    p_value_low = if(first(ctdNum) == "80217") {
      summary(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% filter(chl_comb < 1)))$coefficients[2, 4]
    } else {
      NA_real_
    },
    n_low = if(first(ctdNum) == "80217") {
      sum(chl_comb < 1, na.rm = TRUE)
    } else {
      NA_integer_
    },
    
    .groups = 'drop'
  ) %>%
  mutate(
    # Overall fit statistics
    slope = map_dbl(model, ~coef(.x)[2]),
    intercept = map_dbl(model, ~coef(.x)[1]),
    r2 = map_dbl(model, ~summary(.x)$r.squared),
    adj_r2 = map_dbl(model, ~summary(.x)$adj.r.squared),
    p_value = map_dbl(model, ~summary(.x)$coefficients[2, 4]),
    n = map_int(model, ~length(.x$residuals)),
    
    # Create labels
    label = if_else(
      ctdNum == "80217",
      sprintf("All data:\ny = %.2fx + %.2f\nR² = %.3f (adj. %.3f)\np < %.4f, n = %d\n\n< 1 mg m⁻³:\ny = %.2fx + %.2f\nR² = %.3f (adj. %.3f)\np < %.4f, n = %d",
              slope, intercept, r2, adj_r2, p_value, n,
              slope_low, intercept_low, r2_low, adj_r2_low, p_value_low, n_low),
      sprintf("y = %.2fx + %.2f\nR² = %.3f (adj. %.3f)\np < %.4f\nn = %d",
              slope, intercept, r2, adj_r2, p_value, n)
    )
  )

# Create plot with additional regression line for low values in 80217
f_fit_av %>% 
  filter(use_surface | pres != 1) %>%
  ggplot(aes(x = f_corrected, y = chl_comb)) +
  geom_abline(slope = 1, intercept = 0, 
              linetype = "dashed", color = "gray40", linewidth = 0.8) +
  # Add low range regression line for sensor 80217 (full span)
  geom_abline(data = fit_stats %>% filter(ctdNum == "80217"),
              aes(slope = slope_low, intercept = intercept_low),
              color = "darkred", linewidth = 1.5, alpha = 0.8) +
  geom_smooth(aes(group = id), method = "lm", se = TRUE, 
              linewidth = 0.5, alpha = 0.3, color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, 
              linewidth = 1.5, alpha = 0.3, color = "black") +
  geom_point(size = 3, alpha = 0.6) +
  geom_text(data = fit_stats,
            aes(x = Inf, y = -Inf, label = label),
            hjust = 1.05, vjust = -0.1,
            size = 3.5, fontface = "italic",
            inherit.aes = FALSE) +
  facet_wrap(~ctdNum, scales = "free",
             labeller = labeller(ctdNum = ~paste("Sensor", .))) +
  labs(
    x = "CTD Fluorescence (f_dm)",
    y = expression(paste("Discrete Chlorophyll (mg m"^-3, ")")),
    title = "CTD Fluorescence Calibrations by Sensor",
    subtitle = "Each blue line represents one cast's linear regression; black line shows overall fit; dashed line shows 1:1\nFor sensor 80217: dark red line shows fit for data < 1 mg m⁻³"
  ) +
  theme_bw(base_size = 16) +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = "gray90"),
    strip.text = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 18, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    aspect.ratio = 1
  )

ggsave(here("figures", "fluorescence_calibration_by_sensor_low_80217.png"),
       width = 12, height = 10, dpi = 300)
```

```{r}
# Calculate overall fit statistics for each sensor
fit_stats_lb <- f_fit_av %>% 
  filter(use_surface | pres != 1) %>%
  filter(f_corrected <= 1) %>%
  group_by(ctdNum) %>%
  summarise(
    model = list(lm(chl_comb ~ f_corrected)),
    .groups = 'drop'
  ) %>%
  mutate(
    slope = map_dbl(model, ~coef(.x)[2]),
    intercept = map_dbl(model, ~coef(.x)[1]),
    r2 = map_dbl(model, ~summary(.x)$r.squared),
    adj_r2 = map_dbl(model, ~summary(.x)$adj.r.squared),
    p_value = map_dbl(model, ~summary(.x)$coefficients[2, 4]),
    n = map_int(model, ~length(.x$residuals)),
    label = sprintf("y = %.2fx + %.2f\nR² = %.3f (adj. %.3f)\np < %.4f\nn = %d",
                    slope, intercept, r2, adj_r2, p_value, n)
  )

f_fit_av %>% 
  filter(use_surface | pres != 1) %>%
  filter(f_corrected <= 1) %>% 
  ggplot(aes(x = f_corrected, y = chl_comb)) +
  geom_abline(slope = 1, intercept = 0, 
              linetype = "dashed", color = "gray40", linewidth = 0.8) +
  geom_smooth(aes(group = id), method = "lm", se = TRUE, 
              linewidth = 0.5, alpha = 0.3, color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, 
              linewidth = 1.5, alpha = 0.3, color = "black") +
  geom_point(size = 3, alpha = 0.6) +
  geom_text(data = fit_stats_lb,
            aes(x = -Inf, y = Inf, label = label),
            hjust = -0.05, vjust = 1.1,
            size = 4.5, fontface = "italic",
            inherit.aes = FALSE) +
  facet_wrap(~ctdNum, scales = "free",
             labeller = labeller(ctdNum = ~paste("Sensor", .))) +
  labs(
    x = "CTD Fluorescence (f_dm)",
    y = expression(paste("Discrete Chlorophyll (mg m"^-3, ")")),
    title = "CTD Fluorescence Calibrations by Sensor",
    subtitle = "Each blue line represents one cast's linear regression; black line shows overall fit; dashed line shows 1:1"
  ) +
  theme_bw(base_size = 16) +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = "gray90"),
    strip.text = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 18, face = "bold"),
    plot.subtitle = element_text(size = 14),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    aspect.ratio = 1
  )

ggsave(here("figures", "fluorescence_calibration_by_sensor_lb.png"),
       width = 12, height = 10, dpi = 300)
```

```{r}
# Function to create combined profile + scatter plot for any cast
plot_cast_detailed <- function(cast_id) {
  # Parse date, station, and optionally ctdNum from cast_id
  parts <- strsplit(cast_id, "-")[[1]]
  cast_date <- as.Date(paste(parts[1:3], collapse = "-"))
  cast_station <- parts[4]
  
  # Check if ctdNum is included in cast_id (e.g., "2024-05-15-QU39-670")
  if (length(parts) == 5) {
    cast_ctd <- parts[5]
    profile_data <- f_dm %>%  # Changed from f_corrected
      filter(date == cast_date, station == cast_station, ctdNum == cast_ctd)
    cast_data <- f_fit_multi %>%
      filter(date == cast_date, station == cast_station, ctdNum == cast_ctd)
  } else {
    # Original behavior - get all data for that date/station
    profile_data <- f_dm %>%  # Changed from f_corrected
      filter(date == cast_date, station == cast_station)
    cast_data <- f_fit_multi %>%
      filter(date == cast_date, station == cast_station)
  }
  
  if (nrow(profile_data) == 0) {
    stop(paste("No profile data found for cast:", cast_id))
  }
  
  # Get sensor info
  sensor <- profile_data$ctdNum[1]
  sensor_med <- sensor_medians %>% filter(ctdNum == sensor)
  
  # Get cast info - match on full ID including ctdNum
  cast_info <- model %>% 
    filter(id == cast_id)
  
  # Handle missing cast_info - default to using surface
  if (nrow(cast_info) == 0) {
    warning(paste("Cast", cast_id, "not found in model_flagged. Using surface by default."))
    use_surface <- TRUE
  } else {
    use_surface <- cast_info$use_surface[1]
  }
  
  # Fit model (potentially excluding surface)
  if (use_surface) {
    fit_data <- cast_data %>% filter(!is.na(chl_comb), !is.na(f_corrected))
  } else {
    fit_data <- cast_data %>% filter(!is.na(chl_comb), !is.na(f_corrected), pres != 1)
  }
  
  cast_model <- lm(chl_comb ~ f_corrected, data = fit_data)
  cast_slope <- coef(cast_model)[2]
  cast_intercept <- coef(cast_model)[1]
  r2 <- summary(cast_model)$r.squared
  p_val <- summary(cast_model)$coefficients[2, 4]
  
  # Equation text
  eq_label <- sprintf(
    "Cast: y = %.2fx + %.2f\nMedian: y = %.2fx + %.2f\nR² = %.2f, p = %.4f",
    cast_slope, cast_intercept,
    sensor_med$median_slope, sensor_med$median_intercept,
    r2, p_val
  )
  
  # PROFILE PLOT - using full CTD profile
  profile_plot <- profile_data %>%
    filter(pres < 50) %>%
    ggplot(aes(y = pres)) +
    geom_ribbon(aes(xmin = f_lower - offset, xmax = f_upper - offset),  # Apply offset to ribbons
                fill = "#2E86AB", alpha = 0.3) +
    geom_line(aes(x = f_corrected, color = "CTD Fluorescence"), 
              linewidth = 1, orientation = "y") +
    geom_point(data = cast_data %>% filter(pres < 50),
               aes(x = chl_comb, color = "Discrete Samples"), 
               size = 3, shape = 16) +
    scale_y_reverse(name = "Pressure (dbar)") +
    scale_x_continuous(name = "Chlorophyll (µg/L)") +
    scale_color_manual(
      name = "",
      values = c("CTD Fluorescence" = "#2E86AB", 
                 "Discrete Samples" = "#A23B72"),
      breaks = c("CTD Fluorescence", "Discrete Samples")
    ) +
    theme_bw(base_size = 14) +
    theme(
      legend.position = "top",
      legend.text = element_text(size = 12),
      panel.grid.minor = element_blank(),
      plot.title = element_text(face = "bold", size = 14)
    ) +
    labs(title = "A) Fluorescence Profile")
  
  # Calculate axis limits for scatter
  axis_min <- min(c(cast_data$f_corrected, cast_data$chl_comb), na.rm = TRUE)
  axis_max <- max(c(cast_data$f_corrected, cast_data$chl_comb), na.rm = TRUE)
  buffer <- (axis_max - axis_min) * 0.05
  axis_limits <- c(max(0, axis_min - buffer), axis_max + buffer)
  
  # SCATTER PLOT - only discrete matches
  scatter_plot <- cast_data %>%
    filter(!is.na(chl_comb), !is.na(f_corrected)) %>%
    ggplot(aes(x = f_corrected, y = chl_comb, color = as.factor(pres))) +
    geom_abline(intercept = 0, slope = 1, 
                linetype = "dashed", color = "gray50", linewidth = 0.8) +
    geom_abline(slope = sensor_med$median_slope, 
                intercept = sensor_med$median_intercept,
                color = "blue", linewidth = 1.2, linetype = "solid") +
    geom_smooth(data = fit_data,
                aes(x = f_corrected, y = chl_comb),
                method = "lm", se = TRUE,
                color = "red", fill = "red",
                linewidth = 1, alpha = 0.2,
                inherit.aes = FALSE) +
    geom_point(size = 3) +
    scale_color_brewer(
      name = "Pressure\n(dbar)",
      palette = "Set2"
    ) +
    scale_x_continuous(name = "CTD Fluorescence (µg/L)", 
                       limits = axis_limits) +
    scale_y_continuous(name = "Discrete Chlorophyll (µg/L)", 
                       limits = axis_limits) +
    coord_fixed(ratio = 1) +
    theme_bw(base_size = 14) +
    theme(
      legend.position = "right",
      panel.grid.minor = element_blank(),
      plot.title = element_text(face = "bold", size = 14)
    ) +
    labs(
      title = "B) CTD vs Discrete Chlorophyll",
      subtitle = paste0(
        "Red = cast fit | Blue = sensor median | Dashed = 1:1",
        ifelse(!use_surface, " | Surface excluded", "")
      )
    ) +
    annotate("text", 
             x = axis_limits[1], 
             y = axis_limits[2], 
             label = eq_label,
             hjust = -0.05, 
             vjust = 1.1,
             size = 3.5,
             fontface = "italic")
  
  # COMBINE PLOTS
  combined_plot <- profile_plot + scatter_plot +
    plot_annotation(
      title = paste0("Cast: ", cast_id, " | Sensor: ", sensor),
      theme = theme(plot.title = element_text(size = 16, face = "bold"))
    )
  
  return(combined_plot)
}
```

```{r}
# Usage
plot_cast_detailed("2024-04-12-KC10-211567")

# plot_cast_detailed("2016-08-12-FZH01-18066")
# plot_cast_detailed("2016-08-12-FZH01-1907467")

# plot_cast_detailed("2017-08-31-FZH01-18066")
# plot_cast_detailed("2017-08-31-FZH01-1907467")

# ggsave(here("figures", "qc-2021-04-10-KC10-80217-issue.png"),
#        width = 12, height = 10, dpi = 300)
```


```{r}
# Apply sensor median to few-sample casts
model_few <- f_fit_few %>%
  unite(id, c(date, station), sep = "-", remove = FALSE) %>%
  group_by(date, station) %>%
  summarise(
    ctdNum = first(ctdNum),
    nobs = n(),
    use_surface = any(pres == 1),
    .groups = 'drop'
  ) %>%
  unite(id, c(date, station), sep = "-", remove = FALSE) %>%
  left_join(sensor_medians, by = "ctdNum") %>%
  mutate(
    intercept = NA,
    slope = median_slope,
    r2 = NA,
    p.value = NA,
    quenched = FALSE,
    method = "sensor_median"
  ) %>%
  select(id, date, station, intercept, slope, r2, p.value, 
         nobs, use_surface, quenched, method)
```

```{r}

# First, calculate sensor-level fits and get statistics
sensor_fit_stats_few <- f_fit_few %>%
  filter(!is.na(chl_comb) & !is.na(f_corrected)) %>%
  filter(!id %in% c("2013-05-29-KC10-18032",
                    "2013-07-19-KC10-18032",
                    "2014-05-19-FZH01-18066",
                    "2014-05-19-KC10-18066",
                    "2014-08-01-FZH01-18066",
                    "2014-06-09-FZH01-18066",
                    "2014-06-19-FZH01-18066"
                    )) %>% 
  group_by(ctdNum) %>%
  summarise({
    model_sensor <- lm(chl_comb ~ f_corrected, data = cur_data())
    data.frame(
      slope = round(coef(model_sensor)[2], 3),
      intercept = round(coef(model_sensor)[1], 3),
      r2 = round(summary(model_sensor)$adj.r.squared, 3),
      p.value = round(summary(model_sensor)$coefficients[2, 4], 5),
      n = n()
    )
  }, .groups = 'drop')

# Create the plot with fits and add statistics as text
f_fit_few %>%
  filter(!id %in% c("2013-05-29-KC10-18032",
                    "2013-07-19-KC10-18032",
                    "2014-05-19-FZH01-18066",
                    "2014-05-19-KC10-18066",
                    "2014-08-01-FZH01-18066",
                    "2014-06-09-FZH01-18066",
                    "2014-06-19-FZH01-18066"
                    )) %>%
  ggplot(aes(x = f_corrected, y = chl_comb)) +
  geom_point(alpha = 0.6, size = 2) +
  geom_smooth(method = "lm", se = TRUE, color = "blue", linewidth = 1) +
  geom_text(
    data = sensor_fit_stats_few,
    aes(label = sprintf("y = %.2fx + %.2f\nR² = %.3f\np = %.4f\nn = %d",
                        slope, intercept, r2, p.value, n)),
    x = -Inf, y = Inf,
    hjust = -0.1, vjust = 1.2,
    size = 3,
    family = "mono"  # monospace font for alignment
  ) +
  facet_wrap(~ctdNum, scales = "free") +
  labs(
    x = "CTD Fluorescence (f_dm)",
    y = expression(paste("Discrete Chlorophyll (mg m"^-3, ")")),
    title = "Sensor-Level Fits for Few-Sample Casts (n < 3 per cast)"
  ) +
  theme_bw() +
  theme(
    strip.background = element_rect(fill = "gray90")
  )
```


```{r}
# Combine both datasets for complete statistics
f_fit_combined <- bind_rows(
  f_fit_av %>% 
    filter(use_surface | pres != 1) %>%
    mutate(data_type = "multiple_depths"),
  f_fit_few %>%
    filter(!id %in% c("2013-05-29-KC10-18032",
                      "2013-07-19-KC10-18032",
                      "2014-05-19-FZH01-18066",
                      "2014-05-19-KC10-18066",
                      "2014-08-01-FZH01-18066",
                      "2014-06-09-FZH01-18066",
                      "2014-06-19-FZH01-18066")) %>%
    filter(!ctdNum == 80217) %>% 
    mutate(data_type = "single_depth")
)

# Calculate fit statistics with dual regression for sensor 80217
fit_stats <- f_fit_combined %>% 
  group_by(ctdNum) %>%
  summarise(
    # Overall model (all data)
    model = list(lm(chl_comb ~ f_corrected)),
    
    # Separate fit excluding the two special casts (sensor 80217 only)
    slope_excl = if(first(ctdNum) == "80217") {
      coef(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% 
                filter(!id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217"))))[2]
    } else {
      NA_real_
    },
    intercept_excl = if(first(ctdNum) == "80217") {
      coef(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% 
                filter(!id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217"))))[1]
    } else {
      NA_real_
    },
    r2_excl = if(first(ctdNum) == "80217") {
      summary(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% 
                   filter(!id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217"))))$r.squared
    } else {
      NA_real_
    },
    adj_r2_excl = if(first(ctdNum) == "80217") {
      summary(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% 
                   filter(!id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217"))))$adj.r.squared
    } else {
      NA_real_
    },
    p_value_excl = if(first(ctdNum) == "80217") {
      summary(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% 
                   filter(!id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217"))))$coefficients[2, 4]
    } else {
      NA_real_
    },
    n_excl = if(first(ctdNum) == "80217") {
      sum(!id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217"), na.rm = TRUE)
    } else {
      NA_integer_
    },
    
    # Separate fit for ONLY the two special casts combined (sensor 80217 only)
    slope_april = if(first(ctdNum) == "80217") {
      coef(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% 
                filter(id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217"))))[2]
    } else {
      NA_real_
    },
    intercept_april = if(first(ctdNum) == "80217") {
      coef(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% 
                filter(id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217"))))[1]
    } else {
      NA_real_
    },
    r2_april = if(first(ctdNum) == "80217") {
      summary(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% 
                   filter(id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217"))))$r.squared
    } else {
      NA_real_
    },
    adj_r2_april = if(first(ctdNum) == "80217") {
      summary(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% 
                   filter(id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217"))))$adj.r.squared
    } else {
      NA_real_
    },
    p_value_april = if(first(ctdNum) == "80217") {
      summary(lm(chl_comb ~ f_corrected, data = pick(everything()) %>% 
                   filter(id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217"))))$coefficients[2, 4]
    } else {
      NA_real_
    },
    n_april = if(first(ctdNum) == "80217") {
      sum(id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217"), na.rm = TRUE)
    } else {
      NA_integer_
    },
    
    .groups = 'drop'
  ) %>%
  mutate(
    # Overall fit statistics
    slope = map_dbl(model, ~coef(.x)[2]),
    intercept = map_dbl(model, ~coef(.x)[1]),
    r2 = map_dbl(model, ~summary(.x)$r.squared),
    adj_r2 = map_dbl(model, ~summary(.x)$adj.r.squared),
    p_value = map_dbl(model, ~summary(.x)$coefficients[2, 4]),
    n = map_int(model, ~length(.x$residuals)),
    
    # Create labels
    label = if_else(
      ctdNum == "80217",
      sprintf("All data:\ny = %.2fx + %.2f\nR² = %.3f (adj. %.3f)\np < %.4f, n = %d\n\nExcl. Apr 2020/2021:\ny = %.2fx + %.2f\nR² = %.3f (adj. %.3f)\np < %.4f, n = %d\n\nApr 2020/2021 only:\ny = %.2fx + %.2f\nR² = %.3f (adj. %.3f)\np < %.4f, n = %d",
              slope, intercept, r2, adj_r2, p_value, n,
              slope_excl, intercept_excl, r2_excl, adj_r2_excl, p_value_excl, n_excl,
              slope_april, intercept_april, r2_april, adj_r2_april, p_value_april, n_april),
      sprintf("y = %.2fx + %.2f\nR² = %.3f (adj. %.3f)\np < %.4f\nn = %d",
              slope, intercept, r2, adj_r2, p_value, n)
    )
  )

# Create plot with both data types
f_fit_combined %>% 
  ggplot(aes(x = f_corrected, y = chl_comb)) +
  geom_abline(slope = 1, intercept = 0, 
              linetype = "dashed", color = "gray40", linewidth = 0.8) +
  # Add regression line excluding April casts (sensor 80217)
  geom_abline(data = fit_stats %>% filter(ctdNum == "80217"),
              aes(slope = slope_excl, intercept = intercept_excl),
              color = "darkgreen", linewidth = 1.5, alpha = 0.8) +
  # Add regression line for April casts only (sensor 80217)
  geom_abline(data = fit_stats %>% filter(ctdNum == "80217"),
              aes(slope = slope_april, intercept = intercept_april),
              color = "darkred", linewidth = 1.5, alpha = 0.8) +
  # Individual cast regressions (only for multiple depth data)
  geom_smooth(data = ~filter(., data_type == "multiple_depths"),
              aes(group = id), method = "lm", se = TRUE, 
              linewidth = 0.5, alpha = 0.3, color = "steelblue") +
  # Overall regression (all data)
  geom_smooth(method = "lm", se = TRUE, 
              linewidth = 1.5, alpha = 0.3, color = "black") +
  # Points colored by data type
  geom_point(aes(color = data_type), size = 3, alpha = 0.6) +
  scale_color_manual(
    values = c("multiple_depths" = "gray30", "single_depth" = "orange"),
    labels = c("multiple_depths" = "Multiple depths", "single_depth" = "Single depth"),
    name = "Data type"
  ) +
  geom_text(data = fit_stats,
            aes(x = Inf, y = -Inf, label = label),
            hjust = 1.05, vjust = -0.1,
            size = 3, fontface = "italic",
            inherit.aes = FALSE) +
  facet_wrap(~ctdNum, scales = "free",
             labeller = labeller(ctdNum = ~paste("Sensor", .))) +
  labs(
    x = "CTD Fluorescence (f_dm)",
    y = expression(paste("Discrete Chlorophyll (mg m"^-3, ")")),
    title = "CTD Fluorescence Calibrations by Sensor",
    subtitle = "Blue lines show individual cast regressions; black line shows overall fit including single-depth data; dashed line shows 1:1\nFor sensor 80217: dark green = excluding Apr 2020/2021 casts, dark red = Apr 2020/2021 casts only"
  ) +
  theme_bw(base_size = 16) +
  theme(
    legend.position = "top",
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = "gray90"),
    strip.text = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 18, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    aspect.ratio = 1
  )

ggsave(here("figures", "fluorescence_calibration_by_sensor_with_single_dark.png"),
       width = 12, height = 10, dpi = 300)
```
```{r}
# Create calibration statistics datasheet
calibration_stats <- fit_stats %>%
  select(ctdNum, slope, intercept, r2, adj_r2, p_value, n) %>%
  mutate(
    calibration_type = "standard",
    id_filter = NA_character_
  )

# Add the two special calibrations for sensor 80217
if(any(fit_stats$ctdNum == "80217")) {
  
  # Calibration excluding April casts
  cal_excl <- fit_stats %>%
    filter(ctdNum == "80217") %>%
    select(ctdNum, 
           slope = slope_excl, 
           intercept = intercept_excl, 
           r2 = r2_excl, 
           adj_r2 = adj_r2_excl, 
           p_value = p_value_excl, 
           n = n_excl) %>%
    mutate(
      calibration_type = "excl_april",
      id_filter = "!id %in% c('2020-04-30-KC10-80217', '2021-04-10-KC10-80217')"
    )
  
  # Calibration for April casts only
  cal_april <- fit_stats %>%
    filter(ctdNum == "80217") %>%
    select(ctdNum, 
           slope = slope_april, 
           intercept = intercept_april, 
           r2 = r2_april, 
           adj_r2 = adj_r2_april, 
           p_value = p_value_april, 
           n = n_april) %>%
    mutate(
      calibration_type = "april_only",
      id_filter = "id %in% c('2020-04-30-KC10-80217', '2021-04-10-KC10-80217')"
    )
  
  # Remove standard 80217 and add the two special ones
  calibration_stats <- calibration_stats %>%
    filter(ctdNum != "80217") %>%
    bind_rows(cal_excl, cal_april)
}

# Save to CSV
write_csv(calibration_stats, here("outputs", "ctd_fluorescence_calibrations_stats_2025-11-18.csv"))
```

```{r}
# First, prepare calibration_stats with explicit matching logic
calibration_lookup <- calibration_stats %>%
  mutate(
    # Create flags for the special April casts
    is_april_cast = calibration_type == "april_only",
    is_excl_april = calibration_type == "excl_april"
  )

# Join to f_dm data
f_dm_calibrated <- f_dm %>%
  # First join to get all potential calibrations
  left_join(
    calibration_lookup %>% select(-id_filter),
    by = "ctdNum",
    relationship = "many-to-many"
  ) %>%
  # Filter to keep only the correct calibration for each row
  filter(
    case_when(
      # For sensor 80217 April casts, keep only april_only calibration
      ctdNum == "80217" & id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217") ~ 
        calibration_type == "april_only",
      # For sensor 80217 other casts, keep only excl_april calibration
      ctdNum == "80217" & !id %in% c("2020-04-30-KC10-80217", "2021-04-10-KC10-80217") ~ 
        calibration_type == "excl_april",
      # For all other sensors, keep standard calibration
      ctdNum != "80217" ~ calibration_type == "standard",
      # Default fallback
      TRUE ~ FALSE
    )
  ) %>%
  # Apply the calibration
  mutate(
    f_calibrated = slope * f_corrected 
  ) %>%
  # Clean up temporary columns
  select(-is_april_cast, -is_excl_april)

# Save to CSV
write_csv(f_dm_calibrated, here("outputs", "ctd_offset_slope_corrected_fluorescence_2025-11-19.csv"))
```

```{r}
f_dm_calibrated %>% 
  filter(pres == 5) %>% 
  ggplot(aes(x = date, y = f_calibrated, color = station)) +
  geom_line() +
  geom_point()
```

```{r}
#Use discrete fit on saturated profiles and add them to the time-series as the two identified are important.
```

```{r}
#How do I incorporate quenching?

#Use data with bottle and Fl to develop paradigm - what shapes are quenched vs what are not.
#Tool to flip through this subset of data and decide if loess fit is appropriate.
#Hypothesis is that SCM generally are not quenched, but more mixed profiles are.
#could either develop correction or go back and identify likely quenched profiles.

#I will inevitably need to go through and visualize and classify all profiles.
```

```{r}
# Robust NPQ correction with full profile interpolation
# Step 1: Identify quenched profiles from discrete samples
quenched_profiles <- f_dm_calibrated %>% 
  drop_na(chl_comb, pres) %>% 
  mutate(
    diff = f_calibrated - chl_comb
  ) %>%
  # Identify profiles with quenching in top 5m
  group_by(id) %>%
  summarise(
    is_quenched = any(diff <= -1 & pres <= 5, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(is_quenched) %>%
  pull(id)

# Step 2: Safer LOESS fitting with error handling
fit_loess_safe <- function(data, span = 0.5) {
  # Filter to only bottle data (non-NA chl_comb)
  bottle_data <- data %>% 
    filter(!is.na(chl_comb), !is.na(pres)) %>%
    arrange(pres)
  
  # Need at least 3 points and at least one shallow sample (≤10m)
  if(nrow(bottle_data) < 3) return(rep(NA_real_, nrow(data)))
  if(min(bottle_data$pres) > 10) return(rep(NA_real_, nrow(data)))
  
  # Check for sufficient depth range
  depth_range <- diff(range(bottle_data$pres))
  if(depth_range < 1) return(rep(NA_real_, nrow(data)))
  
  # Adjust span based on number points
  n_points <- nrow(bottle_data)
  adjusted_span <- case_when(
    n_points <= 4 ~ 0.8,
    n_points <= 6 ~ 0.6,
    TRUE ~ span
  )
  
  # Fit LOESS model
  tryCatch({
    loess_model <- loess(chl_comb ~ pres, data = bottle_data, span = adjusted_span)
    
    # Predict within the range of observed data (no extrapolation)
    min_depth <- min(bottle_data$pres)
    max_depth <- max(bottle_data$pres)
    
    predicted <- ifelse(
      data$pres >= min_depth & data$pres <= max_depth & !is.na(data$pres),
      predict(loess_model, newdata = data.frame(pres = data$pres)),
      NA_real_
    )
    
    return(predicted)
  }, error = function(e) {
    return(rep(NA_real_, nrow(data)))
  })
}

# Step 3: Apply correction with proper depth limits
f_dm_calibrated2 <- f_dm_calibrated %>% 
  group_by(id) %>%
  mutate(
    chl_loess = if_else(
      id %in% quenched_profiles,
      fit_loess_safe(cur_data()),
      NA_real_
    )
  ) %>%
  ungroup() %>%
  # Apply NPQ correction only in shallow waters
  mutate(
    diff_ratio = f_calibrated / chl_loess,
    
    # Apply correction with strict depth limit
    f_npq = case_when(
      # Only correct in top 6m where we have good LOESS fit and clear quenching
      pres <= 6 & !is.na(chl_loess) & diff_ratio < 0.8 ~ chl_loess,
      TRUE ~ f_calibrated
    ),
    
    # Flag which points were corrected
    npq_cor = pres <= 6 & !is.na(chl_loess) & diff_ratio < 0.8
  )
```

```{r}
test <- f_dm_calibrated2 %>% 
  filter(npq_cor == T) %>% 
  distinct(id)

test2 <- f_dm_calibrated2 %>% 
  filter(npq_cor == F) %>% 
  distinct(id)
```

```{r}
f_dm_calibrated2 %>% 
  filter(pres < 50) %>%
  filter(id %in% c("2016-09-23-FZH01-18066")) %>% 
  ggplot(aes(y = pres)) +
  geom_line(aes(x = f_dm, color = "CTD Fluorescence"), 
            linewidth = 1, orientation = "y") +
  geom_line(aes(x = f_calibrated, color = "CTD Fluorescence"), 
            linewidth = 1, orientation = "y", color = "green") +
  geom_line(aes(x = f_npq, color = "CTD Fluorescence"), 
            linewidth = 1, orientation = "y", color = "red") +
  geom_point(aes(x = chl_comb, color = "Discrete Samples"),
               size = 3, shape = 16) +
    scale_y_reverse(name = "Pressure (dbar)") +
    scale_x_continuous(name = "Chlorophyll (µg/L)") +
    scale_color_manual(
      name = "",
      values = c("CTD Fluorescence" = "#2E86AB", 
                 "Discrete Samples" = "#A23B72"),
      breaks = c("CTD Fluorescence", "Discrete Samples")
    ) +
    theme_bw(base_size = 14) +
    theme(
      legend.position = "top",
      legend.text = element_text(size = 12),
      panel.grid.minor = element_blank(),
      plot.title = element_text(face = "bold", size = 14)
    ) 
```
```{r}
#profiles to revisit
#2015-05-01-KC10-18032
#Fit to discrete instead?
#2016-03-25-FZH01-18066 
#definitely needed, but maybe deeper.
#2016-09-23-FZH01-18066
#not necessarily bad, but highlights issue with correction.
#2017-08-14-KC10-1907467
#I think something wrong with discrete 
#2018-09-11-FZH01-18066
#Large overestimation due to no 5m - so just extrapolates?
#2019-06-10-KC10-18066
#Not sure on this one - how did this get through QC?
#2020-08-07-KC10-18032
#Does this need more? deeper depth?
#2022-05-05-KC10-18032
#Needs deeper depth? Or CTD issue?
#2022-07-28-KC10-18032
#Doesn't need - does it do backward correct? Causing overestimate.
#2024-04-12-KC10-211567
#Should this be fit to the discrete? 
#2024-05-12-KC10-18032
#Go deeper?
#2025-05-02-KC10-211567
#Doesn't need
```

```{r}
#Really good example of where needed.
#2019-05-11-KC10-18066
#2021-09-05-KC10-18032 
#2016-03-25-FZH01-18066 
```

```{r}
test3 <- f %>% 
  filter(pres < 50) %>%
  filter(date %in% c("2022-05-05"))
```






